{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hedayaahmed/ITNPAI1_Vehicles_Detection/blob/main/2023_Spring_Assignment_AI1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#University of Stirling - Spring 2023\n",
        "\n",
        "## ITNPAI1 - Deep Learning for Vision and NLP (2022/3)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4i5afvUbhmGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. **Problem definition** \n",
        "\n",
        "This project presents deep learning based solution for vehicles detection including cars, motorcycles, buses, and trucks from wild images at street-level. The dataset is collected from two cities which are Cairo and Stirling.\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)\n",
        " "
      ],
      "metadata": {
        "id": "hglJVRRslqMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2. **Dataset creation**\n",
        "202 images from Cairo and 200 images from Stirling are collected using **Mapillary API**. The first step was collecting sequence keys for each city and save them in seperate text file. Then, this [notebook](Hidden_landing_URL) was used to read these keys and send a request to **Mapillary API** to get response and download these images.\n",
        "\n",
        "Cairo dataset includes 809 cars, 49 motorcycles, 15 buses, and 57 trucks, while Stirling dataset contains 548 cars, 2 motorcycles, 7 buses, and 19 trucks.\n",
        "\n",
        "The dataset was annotated using Computer Vision Annotation Tool \"CVAT\". Then, it is exported in Yolo format and converted to PASCAL VOC format using this [notebook](Hidden_landing_URL) because the most recent version of CVAT is more accurate in bounding boxes extraction in Yolo and CVAT formats than in PASCAL VOC. The annotation consists of bounding boxes \"xmin, ymin, xmax, ymax\", object class, and filter the repeated, low quality, or empty images.\n",
        "\n",
        "You must collect a minimum of **200 positive samples** from the study objects for each city (A and B). \n",
        "Note that, depending on the task being solved, it will also be necessary to: \n",
        "\n",
        "   (i) collect more samples - negative ones, for instance;\n",
        "\n",
        "   (ii) annotating each image, delineating objects or creating bounding boxes. Planning and executing this correctly is important to ensure effective training of deep learning-based models.\n",
        "\n",
        "Your dataset can be assembled from one or more of the following ways:\n",
        "\n",
        "  - *M1* - Pictures taken by yourself on site (street view from cities A and B), with attention to anonymization issues (if it is the case). It is not allowed to assemble datasets containing people. Other sensitive patterns, such as license plates, must be properly hidden.\n",
        "\n",
        "  - *M2* - Aerial satellite/drone images obtained from GIS and remote sensing platforms or public repositories. Be careful with unusual file formats that may be challenging to manipulate using basic image processing libraries. We recommend keeping or converting the images to jpg or png.\n",
        "\n",
        "  - *M3* - Pictures taken from other public available datasets. Remember you are not allowed to use datasets containing people or other sensitive patterns/objects.\n",
        "\n",
        "  - *M4* - Images crawled from the internet as a whole (social networks, webpages, etc), with special attention to use and copyrights.\n",
        "\n",
        "  - *M5* - Textual and metadata you may need in your project, with special attention to use and copyrights (as always!).\n",
        "\n",
        "**Important:** If you collect the images on your own or from aerial imagery repositories, it will be necessary to keep the geographic coordinates. If you collect from specific websites, please retain the source links. This information should be placed in a .csv file and made available along with the final dataset.\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "qEgFzxmWrGA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 4. **Dataloader**\n",
        "\n",
        "Here you are required to implement all the code related to pre-processing, cleaning, de-noising and preparing the input images and metadata according to the necessary data structures as input to your pattern recognition module. We recommend using [PyTorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) or [Tensorflow (with Keras)](https://keras.io/getting_started/intro_to_keras_for_engineers/) as a base, but you are free to use any library or platform as long as it is well justified in the [final report](#scrollTo=ws14iV4Dp_vf).\n",
        "\n",
        "[top](scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "EDd6lLwlx4un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your dataloader code here. Create more code cells if you find it necessary\n",
        "!unzip /content/drive/MyDrive/Data_2.zip"
      ],
      "metadata": {
        "id": "RaPd82NmyNCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the files directory and testing directory\n",
        "files_dir = '/content/Data_2/Egypt'\n",
        "test_dir = '/content/Data_2/Stirling'"
      ],
      "metadata": {
        "id": "OKN24aKQ-IVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VehiclesDetectionDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, files_dir, width, height, transforms=None):\n",
        "        self.files_dir = files_dir\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.transforms = transforms  # If transformation is required, when transforms is not None\n",
        "        \n",
        "        self.classes_ = [_, 'car', 'motorcycle', 'bus', 'truck']  # Defining classes, a blank class is given for the background\n",
        "        \n",
        "        self.images = [img for img in sorted(os.listdir(files_dir)) if img[-4:]=='.jpg']\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.files_dir, img_name)\n",
        "        \n",
        "        # Reading the image\n",
        "        img = cv2.imread(img_path)\n",
        "        \n",
        "        # Defining width and height\n",
        "        wt = img.shape[1]\n",
        "        ht = img.shape[0]\n",
        "        \n",
        "        # Converting image to RGB channel and normalizing the image\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img = cv2.resize(img, (self.width, self.height), cv2.INTER_AREA)\n",
        "        img /= 255.0\n",
        "        \n",
        "        annot_name = img_name[:-4] + '.xml'\n",
        "        annot_path = os.path.join(self.files_dir, annot_name)\n",
        "        \n",
        "        # Boxes to store the coordinate points of the bboxes\n",
        "        boxes, labels = [], []\n",
        "        \n",
        "        tree = et.parse(annot_path)\n",
        "        root = tree.getroot()\n",
        "        \n",
        "        # Box coordinates are extracted from the XML files for the given image size\n",
        "        for member in root.findall('object'):\n",
        "            labels.append(self.classes_.index(member.find('name').text))\n",
        "            \n",
        "            xmin = float(member.find('bndbox').find('xmin').text)\n",
        "            xmax = float(member.find('bndbox').find('xmax').text)\n",
        "            ymin = float(member.find('bndbox').find('ymin').text)\n",
        "            ymax = float(member.find('bndbox').find('ymax').text)\n",
        "            \n",
        "            x_min = (xmin/wt)*self.width\n",
        "            x_max = (xmax/wt)*self.width\n",
        "            y_min = (ymin/ht)*self.height\n",
        "            y_max = (ymax/ht)*self.height\n",
        "            \n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "            \n",
        "        # Conversion to Tensors   \n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])  # Calculating area of the boxes\n",
        "        \n",
        "        iscrowd = torch.zeros((boxes.shape[0], ), dtype=torch.int64)\n",
        "        \n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        \n",
        "        image_id = torch.tensor([idx])\n",
        "        \n",
        "        target = {'boxes': boxes, 'area': area, 'labels': labels, \n",
        "                'iscrowd': iscrowd, 'image_id':image_id}\n",
        "        \n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image = img,\n",
        "                                    bboxes = target['boxes'],\n",
        "                                    labels = labels)\n",
        "            \n",
        "            img = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            \n",
        "        return img, target"
      ],
      "metadata": {
        "id": "CZg05ru--MOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observing the dataset without any transformation\n",
        "dataset = VehiclesDetectionDataset(files_dir, 224, 224)\n",
        "print('length of dataset = ', len(dataset), '\\n')"
      ],
      "metadata": {
        "id": "_5u5M1OV_vzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, target = dataset[78]\n",
        "print(img.shape, '\\n',target)"
      ],
      "metadata": {
        "id": "8-YjswhD_v8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_img_bbox(img, target):\n",
        "    \n",
        "    # plot the image and bboxes\n",
        "    # Bounding boxes are defined as follows: x-min y-min width height\n",
        "    fig, a = plt.subplots(1,1)\n",
        "    fig.set_size_inches(5,5)\n",
        "    a.imshow(img)\n",
        "    \n",
        "    label_count = 0\n",
        "    for box in (target['boxes']):\n",
        "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "        rect = patches.Rectangle((x, y),\n",
        "                                 width, height,\n",
        "                                 linewidth = 2,\n",
        "                                 edgecolor = 'r',\n",
        "                                 facecolor = 'none')\n",
        "        if(target['labels'][label_count]) == 1:\n",
        "            write = 'car'\n",
        "        if(target['labels'][label_count]) == 2:\n",
        "            write = 'motorcycle'\n",
        "        if(target['labels'][label_count]) == 3:\n",
        "            write = 'bus'\n",
        "        if(target['labels'][label_count]) == 4:\n",
        "            write = 'truck'\n",
        "        label_count = label_count+1\n",
        "        \n",
        "        a.annotate(write, (box[0]+5, box[1]-5), color='blue', weight='bold', fontsize=10, ha='center', va='center')\n",
        "        # Draw the bounding box on top of the image\n",
        "        a.add_patch(rect)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Bh80TS3y_v_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, target = dataset[25]\n",
        "plot_img_bbox(img, target)"
      ],
      "metadata": {
        "id": "kWWoAmll_5pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, target = dataset[78]\n",
        "plot_img_bbox(img, target)"
      ],
      "metadata": {
        "id": "KiACFoCE_50X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, target = dataset[120]\n",
        "plot_img_bbox(img, target)"
      ],
      "metadata": {
        "id": "ZZlRqwLK_6Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(num_classes, modelName):\n",
        "    \n",
        "    # Loading the pre-trained model\n",
        "    if modelName == 'fastcnn':\n",
        "        model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "        return model\n",
        "    \n",
        "    elif modelName == 'maskcnn':\n",
        "        model = maskrcnn_resnet50_fpn(pretrained=True)\n",
        "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "        return model"
      ],
      "metadata": {
        "id": "0lPtS98UABen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train=True):\n",
        "    if train:\n",
        "        return A.Compose([\n",
        "            A.HorizontalFlip(0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.RandomFog(p=0.5, fog_coef_lower=0.2, fog_coef_upper=0.4),\n",
        "            A.GaussianBlur(p=0.5),\n",
        "            ToTensorV2(p=0.1),     # ToTensorV2 converts image to PyTorch tensor without dividing by 255\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            ToTensorV2(p=0.1),     # ToTensorV2 converts image to PyTorch tensor without dividing by 255\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))"
      ],
      "metadata": {
        "id": "2M9LYwuVABiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_split = 0.2\n",
        "\n",
        "# Loading the training and the testing data with all tghe transformations\n",
        "dataset_train = VehiclesDetectionDataset(files_dir, 480, 480, transforms=get_transform(train=True))\n",
        "dataset_test = VehiclesDetectionDataset(files_dir, 480, 480, transforms=get_transform(train=False))\n",
        "\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# Train test split\n",
        "tsize = int(len(dataset) * test_split) # Getting the splitting index\n",
        "dataset_train = torch.utils.data.Subset(dataset_train, indices[:-tsize])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "\n",
        "# Defining dataloaders\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True,\n",
        "                                              num_workers=4, collate_fn=utils.collate_fn)  # Imported form helper library\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=8, shuffle=True,\n",
        "                                              num_workers=4, collate_fn=utils.collate_fn)"
      ],
      "metadata": {
        "id": "U3AtIeUqABlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 5. **Proposed solution** \n",
        "\n",
        "This is where you should implement most of the code for your solution. Write the routines for training and predicting the models and any necessary intermediate steps. Post-processing functions must also be implemented here.\n",
        "\n",
        "  - Use good programming practices, modularizing and adequately commenting on your code. Code quality will be considered in the final assessment.\n",
        "Again, we recommend using [PyTorch](https://pytorch.org/tutorials/beginner/introyt.html), but you are free to use any library or platform. You just need to justify that in the [final report](#scrollTo=ws14iV4Dp_vf).\n",
        "\n",
        "  - You can use pre-trained models as backbones or any code available on the web as a basis, but they must be correctly credited and referenced both in this notebook and in the final report. Cite the source link repository and explicitly cite the authors of it.\n",
        "If you changed existing code, make it clear what the changes were.\n",
        "Make it clear where your own code starts and where it ends. Note that the originality percentage of the code will be considered in the evaluation, so use external codes wisely and sparingly. **Missconduct alert:** remember that there are many tools that compare existing source code and that it is relatively easy to identify authorship. So, be careful and fair by always properly thanking the authors if you use external code.\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "ScTrpUW8zOp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your proposed solution code here. Create more code cells if you find it necessary\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 5 # Can try by changing to 3 as 1 class is devoted for background\n",
        "num_epochs = 25"
      ],
      "metadata": {
        "id": "jJs8HpW_zX0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_training(modelName, num_epochs, num_classes):\n",
        "    loss = []\n",
        "    loss_classifier = []\n",
        "    loss_box_reg = []\n",
        "    loss_objectness = []\n",
        "\n",
        "    val_map = []\n",
        "    val_map_05 = []\n",
        "\n",
        "    model = get_model(num_classes, modelName)\n",
        "    model.to(device)\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.005)\n",
        "\n",
        "    # Learning rate decreases by 10 every 5 epochs\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        metric = train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=5)\n",
        "        loss.append(metric.meters['loss'].value)\n",
        "        loss_classifier.append(metric.meters['loss_classifier'].value)\n",
        "        loss_box_reg.append(metric.meters['loss_box_reg'].value)\n",
        "        loss_objectness.append(metric.meters['loss_objectness'].value)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        eval, stats = evaluate(model, dataloader_test, device=device)\n",
        "        val_map.append(stats[0])\n",
        "        val_map_05.append(stats[1])\n",
        "    return model, loss, loss_classifier, loss_box_reg, loss_objectness, val_map, val_map_05"
      ],
      "metadata": {
        "id": "IApYGGODBiy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 6. **Experimental tests and evaluations** \n",
        "\n",
        "\n",
        "Here you must implement your code for training, testing and evaluating your solution. For this, the following code blocks (*E1*, *E2*, and *E3*) are mandatory:\n",
        "\n",
        "  - *E1* - Training the models. Implement code to call the dataloaders implemented for training your models.  Make routines to test different parameters of your models. Plot graphs that illustrate how parameters impact model training. Compare. Train and select a model for each city (A and B) and justify. You should use half (50%) of the samples from each dataset for training and leave the other half for testing (50%). \n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "3RBW58of0ZDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your codes for E1 here. Create more code cells if needed\n",
        "fast_rcnn, loss, loss_classifier, loss_box_reg, loss_objectness, val_map, val_map_05 = start_training('fastcnn', num_epochs, num_classes)"
      ],
      "metadata": {
        "id": "jHWwdXg32BEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_epochs = range(1, len(val_map) + 1)\n",
        "plt.plot(x_epochs, val_map, label = \"mAP@0.50:0.95\")\n",
        "plt.plot(x_epochs, val_map_05, label = \"mAP@0.50\")\n",
        "plt.title(\"Validation mAP\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('mAP')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H99xjs40EmQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_epochs = range(1, len(loss_classifier) + 1)\n",
        "plt.plot(x_epochs, loss_classifier, label = \"loss_classifier\")\n",
        "plt.plot(x_epochs, loss_box_reg, label = \"loss_box_reg\")\n",
        "plt.plot(x_epochs, loss_objectness, label = \"loss_objectness\")\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "spB0UF7CEmXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_epochs = range(1, len(loss) + 1)\n",
        "plt.plot(x_epochs, loss, label = \"loss\")\n",
        "plt.title(\"Training Average Losses\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DJ8NNbDuEmh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_nms(prediction, threshold):\n",
        "    # torchvision returns the indices of the boxes to keep\n",
        "    keep = torchvision.ops.nms(prediction['boxes'], prediction['scores'], threshold)\n",
        "    \n",
        "    final_prediction = prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "    \n",
        "    return final_prediction"
      ],
      "metadata": {
        "id": "cN9CaZDSEr0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a torch tensor to a PIL Image\n",
        "def tensorToPIL(img):\n",
        "    return transforms.ToPILImage()(img).convert('RGB')"
      ],
      "metadata": {
        "id": "tDmvgk6PEr3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick one image from the test set\n",
        "img, target = dataset_test[5]\n",
        "\n",
        "# put the model in evaluation mode\n",
        "fast_rcnn.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = fast_rcnn([img.to(device)])[0]\n",
        "    \n",
        "print('predicted #boxes: ', len(prediction['labels']))\n",
        "print('real #boxes: ', len(target['labels']))"
      ],
      "metadata": {
        "id": "tewjfQc5Ezvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('EXPECTED OUTPUT')\n",
        "plot_img_bbox(tensorToPIL(img), target)"
      ],
      "metadata": {
        "id": "9ZoengjIEzyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction['boxes'] = prediction['boxes'].cpu()\n",
        "prediction['labels'] = prediction['labels'].cpu()\n",
        "prediction['scores'] = prediction['scores'].cpu()\n",
        "print('MODEL OUTPUT')\n",
        "plot_img_bbox(tensorToPIL(img), prediction)"
      ],
      "metadata": {
        "id": "7nYA-QmLFAFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nms_preds = apply_nms(prediction, threshold=0.2)\n",
        "print('NMS APPLIED MODEL OUTPUT')\n",
        "plot_img_bbox(tensorToPIL(img), nms_preds)"
      ],
      "metadata": {
        "id": "aSaFaCzeFAIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = VehiclesDetectionDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n",
        "# pick one image from the test set\n",
        "img, target = test_dataset[10]\n",
        "# put the model in evaluation mode\n",
        "fast_rcnn.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = fast_rcnn([img.to(device)])[0]\n",
        "    prediction['boxes'] = prediction['boxes'].cpu()\n",
        "    prediction['labels'] = prediction['labels'].cpu()\n",
        "    prediction['scores'] = prediction['scores'].cpu()\n",
        "print('EXPECTED OUTPUT\\n')\n",
        "plot_img_bbox(tensorToPIL(img), target)\n",
        "print('MODEL OUTPUT\\n')\n",
        "nms_prediction = apply_nms(prediction, threshold=0.01)\n",
        "\n",
        "plot_img_bbox(tensorToPIL(img), nms_prediction)"
      ],
      "metadata": {
        "id": "P9cWP-_JI-iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "qXit19-BJK0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "TWrFlQ75JL6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list = []\n",
        "target_list = []\n",
        "for img, target in test_dataset:\n",
        "  target_list.append(target)\n",
        "  with torch.no_grad():\n",
        "      prediction = fast_rcnn([img.to(device)])[0]\n",
        "      prediction['boxes'] = prediction['boxes'].cpu()\n",
        "      prediction['labels'] = prediction['labels'].cpu()\n",
        "      prediction['scores'] = prediction['scores'].cpu()\n",
        "      pred_list.append(prediction)"
      ],
      "metadata": {
        "id": "sg71IQyeJoQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = MeanAveragePrecision(class_metrics=True)\n",
        "metric.update(pred_list, target_list)\n",
        "pprint(metric.compute())"
      ],
      "metadata": {
        "id": "bXHvwTCgJRN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - *E2* - Testing the models in the dataset. You must implement code routines to test the predictive ability of your models using half of each dataset intended for testing. **The model trained in city A must be tested in city A. The model trained in city B must be tested in city B.** Use the evaluation metrics (accuracy, F1-score, AUC, etc) that are most appropriate for your problem. Plot graphs that illustrate the results obtained for each city (A and B). Plot visual examples of correctly (true positive) and incorrectly (false positive) classified samples. \n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)\n"
      ],
      "metadata": {
        "id": "TunTimEv1szf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your codes for E2 here. Create more code cells if needed\n",
        "\n"
      ],
      "metadata": {
        "id": "_s6ygCpi2CO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - *E3* - Testing the models crossing datasets. Here you must do exactly the same as in *E2*, but now training in one city and testing in the other. **The model trained in city A must be tested in city B. The model trained in city B must be tested in city A.** Use the same metrics and plot the same types of graphs so that results are comparable.\n",
        "\n",
        "[top](scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "bZ0zVXjQ1x0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your codes for E3 here. Create more code cells if needed\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cotguzxyo3Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 7. **Quiz and Report**\n",
        "\n",
        "Answer the assessment quiz that will be made available on Canvas one week before the final deadline. Make a 2-page latex report using the [IEEE template](https://www.overleaf.com/read/hkfsjjsxmxcn) with a maximum of 1000 words. You can deliver the report in MS Word if you prefer. Your report should contain five sections: introduction, description of the proposed solution with justifications, results (here you can include the same graphs and pictures generated in this jupyter notebook), discussion of the results, and conclusion. Properly cite references to articles, tutorials, and sources used. A pdf version of your report should be made available in the project's github repository under the name \"[project name] + _final_report.pdf\".\n",
        "\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "ws14iV4Dp_vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# 8. **Demonstration**\n",
        "\n",
        "Some projects (around 10%) will be selected for a mandatory demonstration. During the demo, you will be asked about implementation details and decisions that led to the design of the developed solution.\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "ShLJrPKT4SQp"
      }
    }
  ]
}